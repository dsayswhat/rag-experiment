The harder technical problem is to figure out a way to extract text from unstructured PDFs. And I'm seeing early indications that we might be able to do. A pretty good job of extracting text from PDFs using python libraries like Pimo PDF. And Pima PDF for llm. These things dissect a PDF at the at the file structure level. They look at the lines they look at the font treatments, and they're able to infer things like columns. And. Page structure and try to get out semantic content. But as I use it out of the box, it dumping, like running a pass with those tools straight to markdown, produces. Intelligible results. But not results that are good enough. Put into a migration pipeline because the resulting content in text or in markdown is messy. Also, maybe markdown is not the best format for this, maybe Json or some other format is more appropriate. So, Primo PDF and its related libraries? If you use them, not as just a little black box where you plug in a PDF and turn the cranking out comes the stuff, and that's what you get. If you spend more time developing with them, you can actually. I believe we're going to be able to. Uh, provide. Additional business intelligence. And then. Drive the Pima PDF. Libraries in such a way that we are able to extract content. Maybe like on a per PDF basis, where we we look at how the file is structured, and how the text treatments are and a human sort of infers. By way of a report that tells us, what are the available tax treatments and how, how is the white space? Breaking down. What kind of structures do we see in the in the PDF? Um, being able then to? Run a report possibly via a command line tool or eventually via some kind of UI. Although it's that's not clear yet, but at the late first, I'm working on the command line writing utilities that report on what's in the PDF, so the human can look at the PDF and look at the report side by side and provide judgment about. To treat that PDF. And what's what I'm looking to do next is find a way to express? The. That judgment of a person like this particular text treatment. In a certain color in a certain font size that I've detected in the PDF through my reporting is. Itself, an H1 or an H2. And if you can Define? Uh, font, size, and color. Or some other indicator. That's clear because of the reporting that we did through Pima PDF. We should be able to then feed that back into the system and give. Um, additional context by which a custom script can be written. Possibly in the form of. Like a data structure that contains that header information. Those criteria for what is a header? So that when we feed it into our script, we have additional contacts, besides, maybe the simpler, more generic versions of the Pimopdf or llm is using. So, in that case, then we would be able to have a human input into file structure and layout. And what it means to? An inset box with a certain background color and a certain text treatment. If we could put labels on those things and feed that in in some way that? Can be turned into. Uh, testable criteria. We would maybe be entering into a domain specific. Language or a domain specific configuration. Per PDF or family of PDFs that could be used to extract, not just streams of text, but streams of text that are. Um. That were selected because we were aware of the heading structure and the context of the PDF. And then those things could be chunked. And related to one another in such a way that? Uh, continuity could be kept, you know, multiple pieces of content over multiple pages could be related to one another in the end result through some sort of data structure. And we'd be able then to reassemble that into the CMS. With. Uh, again, some human intervention and some AI or or scripting. Uh, tools to. To reassemble these things in such a way that? Uh, you know, the business goal of taking a PDF? Reducing it to its essential semantic. Um, headers and structure table extraction. Image extraction like, we break the PDF down into. Parts that still retain, meaning not just cutting chunks of text in half because they're too long or whatever. And then are able to reassemble them into whatever the new, uh. And now, the new content model is. Whether that's taking a bunch of chunks of text and reassembling them into a new page structure within the PDF, whether it's breaking down the. Uh, the PDF. Into multiple separate Pages, not on the basis of the PDF's original pages, but into whatever the new page structure is in the CMS. Example Drupal's book module, which allows hierarchy to be a parent child, outlying relationships. We might need to break the PDF down. On a semantic structural basis, not on a page by page basis, and end up with new pages based on that semantic and structural breakdown. That is. What I'm hoping to get to is the ability to run a report, get information about structure, use a human judgment to. To select the parts of the structure that are leading to meaning and then feed that back into a script. Takes that domain in terms of like that specific PDF takes that domain specific set of patterns. Headers, inset boxes, tabular structures, and then calls a script or a series of scripts that uses the right tool to do text extraction. Image extraction table extraction, which might be separate python. Libraries like Pymo PDF might be not the best choice for table extraction, for example. Doing all that. In such a way that we end up with a. Data structure, which can be fed into a migration process so that. Unstructured PDFs become. Either fully or partially structured, and then. Um, that can be fed into mapped into a CMS content model as like the ones we do in Drupal. So, that is, my hope and my vision for this. And I'm gonna be working the problem and seeing if I can get to it. The implication at lullabot would be that we could say yes to a client that comes to us with. A thousand or thousands of PDFs? We could Envision a workflow and a pipeline by which we. Um. Take a large portion of those PDFs and extract meaningful content from them. Isolating problem cases that are manual. And applying our same. Uh, highly skilled project management technique. Not just HTML content and web pages, but also to PDFs so that we can transfer them into web pages that are more accessible. And relieve the client of a lot of the manual burden of having to remediate this stuff. And my hope is that this pipeline of analysis and extraction would lead to high quality Source material for Content migrations.